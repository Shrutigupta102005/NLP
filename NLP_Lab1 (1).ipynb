{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9b0_iH7uGzX",
        "outputId": "560b955b-9cc1-401c-bcc0-4087ce4a14a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Synonym replacement\n",
        "import random  # used to select random words or action\n",
        "import nltk   # imports the natural lang toolkit\n",
        "from nltk.corpus import wordnet # wordnet is a lexical database of eng words\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt') # used for tokenization\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger') # Often needed for wordnet synsets depending on usage\n",
        "nltk.download('punkt_tab') # Specifically address the LookupError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"the movie was absoluetly fantastic and enjoyable\""
      ],
      "metadata": {
        "id": "-nCT64QruRHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    # Initialize an empty set to store synonyms\n",
        "    synonyms = set()\n",
        "\n",
        "    # Iterate through each synset (set of synonyms) for the given word\n",
        "    for syn in wordnet.synsets(word):\n",
        "        # Iterate through each lemma (a base form of a word) in the synset\n",
        "        for lemma in syn.lemmas():\n",
        "            # Get the name of the lemma, replace underscores with spaces, and add to the set\n",
        "            synonyms.add(lemma.name().replace('_', ' '))\n",
        "\n",
        "    # Return a list of unique synonyms, excluding the original word itself\n",
        "    # Convert to lowercase for case-insensitive comparison\n",
        "    return [syn for syn in list(synonyms) if syn.lower() != word.lower()]\n",
        "\n",
        "\n",
        "def synonym_replacement(text, n):\n",
        "    # Tokenize the input text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Create a copy of the original word list\n",
        "    new_words = words.copy()\n",
        "\n",
        "    # Get a list of unique words in the text that are alphabetic\n",
        "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
        "    # Shuffle the list of words to replace randomly\n",
        "    random.shuffle(random_word_list)\n",
        "\n",
        "    # Initialize a counter for the number of words replaced\n",
        "    num_replaced = 0\n",
        "\n",
        "    # Iterate through the shuffled list of words\n",
        "    for word in random_word_list:\n",
        "        # Get the synonyms for the current word\n",
        "        synonyms = get_synonyms(word)\n",
        "        # If synonyms are found for the word\n",
        "        if synonyms:\n",
        "            # Choose a random synonym from the list of synonyms\n",
        "            synonym = random.choice(synonyms)\n",
        "            # Find the index of the original word in the words list\n",
        "            word_index = words.index(word)\n",
        "            # Replace the word at that index with the chosen synonym in the new_words list\n",
        "            new_words[word_index] = synonym\n",
        "            # Increment the counter for replaced words\n",
        "            num_replaced += 1\n",
        "\n",
        "        # If the desired number of replacements has been reached, break the loop\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    # Join the words in the new_words list back into a sentence\n",
        "    sentence = ' '.join(new_words)\n",
        "    # Return the sentence with synonym replacements\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "79-6FP6Avfu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c1f50ba",
        "outputId": "0d60d039-a42f-45c4-82b2-25cadc875862"
      },
      "source": [
        "orignal =  \"the movie was absoluetly fantastic and enjoyable\"\n",
        "augmented = synonym_replacement(orignal , n=2)\n",
        "print(orignal)\n",
        "print(augmented)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie was absoluetly fantastic and enjoyable\n",
            "the movie was absoluetly terrific and gratifying\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def diagram_flip(text):\n",
        "  words = nltk.word_tokenize(text)\n",
        "  new_words = words.copy()\n",
        "  indices = list(range(len(words)-1))\n",
        "  if not indices:\n",
        "    return text\n",
        "\n",
        "  file_index = random.choice(indices)\n",
        "  new_words[file_index], new_words[file_index+1] = new_words[file_index+1], new_words[file_index]\n",
        "  return ' '.join(new_words)\n"
      ],
      "metadata": {
        "id": "w-nnJaKZ1gLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orignal =  \"the movie was absoluetly fantastic and enjoyable\"\n",
        "augmented = diagram_flip(orignal)\n",
        "print(orignal)\n",
        "print(augmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg80H9w12aZv",
        "outputId": "537a3110-033e-4fe8-a652-1ca44015532e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie was absoluetly fantastic and enjoyable\n",
            "movie the was absoluetly fantastic and enjoyable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# back Translation : translate text to another another lang  and then back to orignal\n",
        "!pip install deep_translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkepjIS-2_ko",
        "outputId": "1f3cf76d-4d6e-45aa-a8ea-3f8eeae85943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.7.14)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "def back_translate_verbose(text , intermediate_lang = 'fr'):\n",
        "  try:\n",
        "    translated = GoogleTranslator(source = 'auto' , target=intermediate_lang).translate(text)\n",
        "    back_translated = GoogleTranslator(source = intermediate_lang , target='en').translate(translated)\n",
        "    print(f\"Orignal: {text}\")\n",
        "    print(f\"Intermediate: {translated}\")\n",
        "    print(f\"Back translated: {back_translated}\")\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return text"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8HHgL0Fo3oID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orignal =  \"the movie was absoluetly fantastic and enjoyable\"\n",
        "augmented = back_translate_verbose(orignal)\n",
        "print(orignal)\n",
        "print(augmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeBxvOxH31Yq",
        "outputId": "b291db50-0fee-43db-fb68-c4d5ec1cb1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal: the movie was absoluetly fantastic and enjoyable\n",
            "Intermediate: Le film était absolument fantastique et agréable\n",
            "Back translated: The film was absolutely fantastic and pleasant\n",
            "the movie was absoluetly fantastic and enjoyable\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Adding Noise\n",
        "# random character swaps\n",
        "import random\n",
        "\n",
        "def add_noise(text,noise_level = 0.17):\n",
        "  text_chars  = list(text)\n",
        "  num_noisy = int(noise_level * len(text_chars) * noise_level)\n",
        "  for _ in range(num_noisy):\n",
        "    idx = random.randint(0,len(text_chars)-2)\n",
        "    text_chars[idx], text_chars[idx+1] = text_chars[idx+1], text_chars[idx]\n",
        "  return ''.join(text_chars)\n",
        "\n"
      ],
      "metadata": {
        "id": "r_pZ2QYi5Yvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orignal =  \"the movie was absoluetly fantastic and enjoyable\"\n",
        "augmented = add_noise(orignal)\n",
        "print(orignal)\n",
        "print(augmented)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqgqYKgk62Kr",
        "outputId": "096db164-6e16-492d-c996-4fe34db960df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie was absoluetly fantastic and enjoyable\n",
            "the movie was absoluetly fantasti cand enjoyable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dy55ob4G66PC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}